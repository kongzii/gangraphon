# GanGraphon

## Repository

### Structure

- common
  - pgd.py - script to calculate Parameterized Graphlet Decomposition from graph using the [PGD library](https://github.com/nkahmed/PGD)
  - evaluators **
    - eval.py - script to evaluate graphs against original with TDAEvaluation
    - pertub.py - script to pertubate original graph and plot TDAEvaluation and networkx statistics
    - pertubations.py - code from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`
    - tda.py - code from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`
  - data.py - datasets classes
  - graphons.py - implementations of artificial graphon functions
  - tools.py - various helper functions
  - pickle_graphon.py - helper script to generate graphs from artifical graphon and save as a pickle file compatible with GRAN and GraphRNN
  - render_table.py  - utility script to render latex tables from TDA evaluations
- data - directory that will contain training statistics, checkpoints, similarity tests, etc.
- gangraphon
  - train.py - script to train the GanGraphon
  - models.py - model implemented using PytorchLightning and PytorchGeometric
- classification - extra experiments to evaluate the model, see help messages of `train_finetune.py` and `train_separatetest.py`
- pyper **
- tmp - directory where some calculation may be persisted for later usage
- repos
  - gran_repackaged - [GRAN](https://github.com/lrjconan/GRAN) implementation, changes only to be able to train from our pickled graphs
  - graphrnn - [GraphRNN](https://github.com/snap-stanford/GraphRNN) implementation, changes only to be able to train from our pickled graphs

** Confident codes from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions` that were shared with us, training will work, but some tests will be skipped without them.

### Running experiments

Docker environment is prepared to replicate results as easily as possible.

To build the image, run

```bash
USER_ID="$(id -u)" GROUP_ID="$(id -g)" docker-compose build
```

## Training GanGraphon

To start training, simply run

```bash
docker-compose run base python gangraphon/train.py --tudataset-name COLLAB
```

You can aslo train only on specific class of TUDataset

```bash
docker-compose run base python gangraphon/train.py --tudataset-name COLLAB --class-idx 0
```

You can see all options by running

```bash
docker-compose run base python gangraphon/train.py --help
```

## Train GRAN and GraphRNN

This two source codes were obtained from their official repositories and modified as little as possible, only to allow training from our dataset. Instead of Docker, they are using Poetry to manage the dependencies.

After the GanGraphon training, pickled file with training and testing graphs will be saved. This allows us to train and evaluate other models on exact same datasets as GanGraphon.

To train the GraphRNN, change directory to it and specify path to the dataset

```bash
cd repos/graphrnn
poetry run train_graphrnn --dataset-file path_to_the_pickled_dataset.pickle
```

The GRAN training is very similar, but we also need to specify the model configuration file

```bash
cd repos/gran_repackaged
poetry run train_gran --dataset-file path_to_the_pickled_dataset.pickle --config gran_repackaged/config/config.yaml
```

## `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`

This code was shared with us confidently, thus it will not be publicly available.

All training will work without it, just `TDAEvaluation` from the paper will be skipped.

### Pertubate

To pertubate original graph to see how various tests are affected, run the `pertub.py` script.

You can pertub one graph defined as edge list

```bash
docker-compose run base python common/evaluators/pertub.py \
  --graph-file data/string_db/abiotrophia/592010.protein.links.full.v11.0.abiotrophia.txt
```

Or artifical graphs generated by a Graphon

```bash
docker-compose run base python common/evaluators/pertub.py \
  --graphon-name spades
```

Or any dataset from TUDataset

```bash
docker-compose run base python common/evaluators/pertub.py \
  --tudataset-name COLLAB
```

This will generate `pertubations.tda.json` and `pertubations.tda.jpg` so you can see which TDAEvaluation best captures changes in the graph.

### Evaluate

Evaluation script is able to compare ground truths graphs from various sources with generates graphs from various sources using the TDAEvaluation.

We can run the `docker-compose run base python common/evaluators/eval.py` script and as a ground true graphs specify either:

- `--graph-file` for single graph from specified as edge list
- `--graphon-name` for artifical graphs generator from graphon with given name
- `--pickled-orig` for graphs in `nx.Graph` formad saved as a list in the pickle file

And these can be compared with generated graphs, that can be specified as:

- `--sec-graphon-name` to compare against graphs generator from another artifical graphon
- `--pickled` to compare against graphs saved in single pickle file
- `--pickled-folder` to search for pickle files in this folder and compare against all graphs in them

For another options, see the help message  `docker-compose run base python common/evaluators/eval.py --help`.
