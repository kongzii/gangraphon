# DeepGraphon

## Repository

### Structure

- common
  - pgd.py - script to calculate Parameterized Graphlet Decomposition from graph using the [PGD library](https://github.com/nkahmed/PGD)
  - evaluators **
    - eval.py - script to evaluate DeepGraphon and ERGM generated graphs against original with TDAEvaluation
    - pertub.py - script to pertubate original graph and plot TDAEvaluation and networkx statistics
    - pertubations.py - code from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`
    - tda.py - code from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`
  - data.py - datasets classes
  - tools.py - various helper functions
  - pickle_graphon.py - helper script to generate graphs from artifical graphon and save as a pickle file compatible with GRAN and GraphRNN
- deepgraphon
  - train.py - script to train the DeepGraphon
  - models.py - model implemented using PytorchLightning
  - tests.py - script and utilities to calculate various networkx statistics
- data - directory that will contain training statistics, checkpoints, similarity tests, etc.
- pyper **
- tmp - directory where some calculation may be persisted for later usage

** Confident codes from `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions` that were shared with us, training will work, but some tests will be skipped without them.

### Running experiments

Docker environment is prepared to replicate results as easily as possible.

To build the image, run

```bash
USER_ID="$(id -u)" GROUP_ID="$(id -g)" docker-compose build
```

## Training DeepGraphon

You can see all options by running

```bash
docker-compose run base python deepgraphon/train.py --help
```

To start training, simply run

```bash
docker-compose run base python deepgraphon/train.py --graph-file data/networkrepository/bn-mouse_brain_1/bn-mouse_brain_1.edges
```

If `PGD` statistics are found, they are used for the training, otherwise simpler naive monte carlo method will be used to calculate homomorphism densities.

## `Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions`

This code was shared with us confidently, thus it will not be publicly available.

All training will work without it, just `TDAEvaluation` from the paper will be skipped during tests.

### Pertubate

To pertubate original graph to see how various tests are affected, run the `pertub.py` script.

You can pertub one graph defined as edge list

```bash
docker-compose run base python common/evaluators/pertub.py \
  --graph-file data/networkrepository/aves-wildbird-network/aves-wildbird-network.edges
```

Or artifical graphs generated by a Graphon

```bash
docker-compose run base python common/evaluators/pertub.py \
  --graphon-name spades
```

Or any dataset from TUDataset

```bash
docker-compose run base python common/evaluators/pertub.py \
  --tudataset-name COLLAB
```

This will generate `pertubations.tda.json` and `pertubations.tda.jpg` so you can see which TDAEvaluation best captures changes in the graph.

### Evaluate

Evaluation script is able to compare ground truths graphs from various sources with generates graphs from various sources using the TDAEvaluation.

We can run the `docker-compose run base python common/evaluators/eval.py` script and as a ground true graphs specify either:

- `--graph-file` for single graph from specified as edge list
- `--graphon-name` for artifical graphs generator from graphon with given name
- `--pickled-orig` for graphs in `nx.Graph` formad saved as a list in the pickle file

And these can be compared with generated graphs, that can be specified as:

- `--sec-graphon-name` to compare against graphs generator from another artifical graphon
- `--checkpoint` to generate graphs from saved model checkpoint
- `--pickled` to compare against graphs saved in single pickle file
- `--pickled-folder` to search for pickle files in this folder and compare against all graphs in them

For another options, see the help message  `docker-compose run base python common/evaluators/eval.py --help`.
